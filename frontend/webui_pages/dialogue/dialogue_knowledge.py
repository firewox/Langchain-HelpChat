# @Time  :2024/12/7 11:33
# @Author: yutian.li
# @Email : lyutian2020@qq.com
import streamlit as st
from frontend.api_frontend.api_frontend_knowledge_chat import api_frontend_knowledge_chat
from frontend.api_frontend.api_frontend_chat import api_frontend_chat
from utils.logger import logger
from backend.configs.model_config import PROMPT_TEMPLATE_FLAG
from configs.project_configs import ASSISTANT_LOGO_IMG,USER_LOGO_IMG


# ç”¨æˆ·ç¬¬ä¸€æ¬¡è¿›åˆ°é¡µé¢ï¼Œè®¾falg=Trueï¼Œå°†å†å²èŠå¤©è®°å½•æŒ‚è½½åˆ° st.session_state.all_messages é‡Œ
class dialogue_knowledge_util:
    FLAG=True
    COUNT=0

# å®ä¾‹åŒ–apiè¯·æ±‚æ¥å£
api_frontend_chat_instance:api_frontend_chat = api_frontend_chat()
api_frontend_knowledge_chat_instance:api_frontend_knowledge_chat = api_frontend_knowledge_chat()

# new UI layout
def dialogue_knowledge_page():
    # ä½¿ç”¨ session_state æ¥å­˜å‚¨å½“å‰çš„é€‰æ‹©
    if 'model_name' not in st.session_state or "model_list" not in st.session_state:
        # ç»™åç«¯å‘è¯·æ±‚ï¼ŒæŸ¥è¯¢å¯ç”¨çš„å¤§æ¨¡å‹
        model_list = api_frontend_chat_instance.get_model_list()
        st.session_state.model_name = model_list[0]
        st.session_state.model_list = model_list
    if "history_name" not in st.session_state or "history_name_list" not in st.session_state:
        # ç»™åç«¯å‘é€è¯·æ±‚ï¼ŒæŒ‚è½½å†å²èŠå¤©è®°å½•çš„åç§°åˆ—è¡¨
        history_name_list = api_frontend_chat_instance.get_history_name_list()
        st.session_state.history_name = history_name_list[0]
        st.session_state.history_name_list = history_name_list
        # Initialize chat history
        # # ç»™åç«¯å‘è¯·æ±‚ï¼ŒæŒ‚è½½å†å²èŠå¤©è®°å½•
        messages = api_frontend_chat_instance.get_history_chat(history_name=st.session_state.history_name)
        logger.info(f"\n\næŒ‚è½½å†å²èŠå¤©è®°å½•A={messages}")
        #st.session_state.messages = messages
    if "knowledge_name" not in st.session_state or "knowledge_name_list" not in st.session_state:
        knowledge_name_list = api_frontend_knowledge_chat_instance.get_knowledge_name_list()
        st.session_state.knowledge_name = knowledge_name_list[0]
        st.session_state.knowledge_name_list = knowledge_name_list
    if "embedding_model_name" not in st.session_state or "embedding_model_name_list" not in st.session_state:
        embedding_model_name_list = api_frontend_knowledge_chat_instance.get_embedding_model_list()
        st.session_state.embedding_model_name = embedding_model_name_list[0]
        st.session_state.embedding_model_name_list = embedding_model_name_list
    if len(api_frontend_chat_instance.messages)==0:
        # # ç»™åç«¯å‘è¯·æ±‚ï¼ŒæŒ‚è½½å†å²èŠå¤©è®°å½•
        messages = api_frontend_chat_instance.get_history_chat(history_name=st.session_state.history_name)
        #st.session_state.messages = messages

    col1,col2,col3,col4,col5 = st.columns([1,1,1,1,1])
    col1_placeholder = col1.empty()
    col2_placeholder = col2.empty()
    col3_placeholder = col3.empty()
    col4_placeholder = col4.empty()
    col5_placeholder = col5.empty()
    #
    #dialogue_knowledge_util.COUNT+=1
    # col1
    genre = col1_placeholder.selectbox(
        ":rainbow[LLM Select]",
        (f"{ml}" for ml in st.session_state.model_list),
    )
    api_frontend_chat_instance.model_name = genre
    # col2
    genre2 = col2_placeholder.selectbox(":rainbow[History Select]",(hnl for hnl in st.session_state.history_name_list),)
    if genre2!=st.session_state.history_name:
        api_frontend_chat_instance.history_name = genre2
        st.session_state.history_name = genre2
        # # ç»™åç«¯å‘è¯·æ±‚ï¼ŒæŒ‚è½½å†å²èŠå¤©è®°å½•
        messages = api_frontend_chat_instance.get_history_chat(history_name=st.session_state.history_name)
        #st.session_state.messages=messages
        logger.info(f"\n\næŒ‚è½½å†å²èŠå¤©è®°å½•B={messages}")
    # col3
    # col4
    genre4 = col4_placeholder.selectbox(":rainbow[Knowledge Books]",(i for i in st.session_state.knowledge_name_list),)
    api_frontend_knowledge_chat_instance.knowledge_name = genre4
    st.session_state.knowledge_name = genre4
    # col5
    genre5 = col5_placeholder.selectbox(":rainbow[Embedding Models]",(i for i in st.session_state.embedding_model_name_list),)
    api_frontend_knowledge_chat_instance.embedding_model_name = genre5
    st.session_state.embedding_model_name = genre5


    st.title("ğŸ¦œğŸ”— HelpChat App")
    # Display chat messages from history on app rerun
    if len(api_frontend_chat_instance.messages)>=2:
        # TODO å¤„ç† streamlit çš„bugé—®é¢˜ï¼šåœ¨æœ¬è„šæœ¬ä¸­ï¼Œå…³äºmessagesæ•°æ®çš„å¢åŠ å’Œæ˜¾ç¤ºè¢«å¹²æ‰°ï¼Œåç«¯ä¼šå¯¹queryè¿›è¡Œå¤„ç†ï¼šè¿›è¡Œ prompt = query+rag çš„æ“ä½œï¼Œ
        # TODO ä½†æ˜¯å‰ç«¯ streamlité¡µé¢çš„ messagesä¼šè¢«è‡ªåŠ¨æ³¨å…¥ prompt=query+ragã€‚
        #  ä¾‹å¦‚ï¼šæœ¬æ¥çš„queryä¸ºmessages={â€œroleâ€:user,"content":"ä½ å¥½"}ï¼Œç»è¿‡åç«¯å¤„ç†åï¼Œå‰ç«¯çš„messagesè‡ªåŠ¨å˜ä¸º messages={â€œroleâ€:user,"content":"å·²çŸ¥ä¿¡æ¯ï¼šRAGçŸ¥è¯†ï¼Œé—®é¢˜æ˜¯ï¼šä½ å¥½"}
        # TODO ç»è¿‡å¤šç§å°è¯•è§£å†³ï¼Œæš‚æ—¶è§£å†³ä¸äº†
        for i in api_frontend_chat_instance.messages:
            if i.get("role") == "user" and PROMPT_TEMPLATE_FLAG in i.get("content"):
                index = i.get("content").rfind(PROMPT_TEMPLATE_FLAG)
                i["content"] = i.get("content")[index+len(PROMPT_TEMPLATE_FLAG):]
        dialogue_knowledge_util.COUNT+=1
        #logger.info(F"\ndialogue_knowledge_util.COUNT={dialogue_knowledge_util.COUNT}\nmessages={st.session_state.messages}")
        #st.info(f"st.session_state.all_messages={st.session_state.all_messages}")
        logger.info(f"\n\nA count={dialogue_knowledge_util.COUNT},\nmessages={api_frontend_chat_instance.messages}")
        for index,message in enumerate(api_frontend_chat_instance.messages[2:]):#(st.session_state.messages):
            with st.chat_message(name=message['role'],avatar=":material/face:"):
                st.markdown(message["content"])

    # Accept user input
    prompt = st.chat_input("æœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿ")
    if prompt:
        #st.session_state.messages.append({"role": "user", "content": prompt})
        logger.info(f"\n\n###queryæµ‹è¯•A={prompt}")
        api_frontend_chat_instance.messages = api_frontend_chat_instance.messages + [{"role": "user", "content": prompt}]
        logger.info(f"\n\n###queryæµ‹è¯•B={api_frontend_chat_instance.messages}")
        # Display user message in chat message container
        with st.chat_message(name="user",avatar=":material/face:"):
            st.markdown(prompt)

        # Display assistant response in chat message container
        with st.chat_message(name="assistant",avatar=":material/smart_toy:"):
            if len(api_frontend_chat_instance.messages)==0:
                pass
            else:
                # ç»™åç«¯å‘è¯·æ±‚ï¼Œå‘é€æ¶ˆæ¯
                stream = api_frontend_chat_instance.get_knowledge_chat(model_name=st.session_state.model_name,
                                                                       embedding_model_name=st.session_state.embedding_model_name,
                                                                       knowledge_name=st.session_state.knowledge_name,
                                                                       msgs=api_frontend_chat_instance.messages.copy())
                response = st.write_stream(stream)
                
                # ä¿å­˜è¿”å›çš„èŠå¤©å“åº”
                #st.info(f"messages={get_history_chat.messages}")
                #st.session_state.messages.append({"role":"assistant","content":response})
                api_frontend_chat_instance.messages = api_frontend_chat_instance.messages + [{"role": "assistant", "content": response}]
                #st.session_state.messages.append({"role":"assistant","content":response})
                # ç»™åç«¯å‘è¯·æ±‚ï¼Œå®æ—¶ä¿å­˜èŠå¤©è®°å½•
                api_frontend_chat_instance.set_history_chat(messages=api_frontend_chat_instance.messages)

